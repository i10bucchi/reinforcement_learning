# reinforcement_learning

## Overview

機械学習の勉強の一環でSARSAとQLearningをC言語で作成しました.

## Description

SARSAとQLearningで同一の崖歩き問題を解くことで両者の違いを確認します.

### SARSAとQLearningの違い

- SARSAはQ値の更新時に次のステップでの状態と行動を参照
- QLearningはQ値の更新時に次のステップの状態下で最もQ値が高くなる行動を参照

### 崖歩き問題

課題の設定(図1を参照)

- 縦11マス＊横19マスの2次元離散空間においてスタート地点Sからゴール地点Gへ向かう。
- 11マス＊19マスの「外側」は全て「壁」とし、「壁」方向へ移動した場合、エージェントの位置は変更せず、報酬「ー100」を与える。
- ＝例えば、スタート地点Sにおいて、下方向と左方向のマスはそれぞれ「壁」となる。
- 移動可能領域のマスを1マス移動する毎に報酬「−1」を与える。
- ゴール地点のマスに辿り着いた場合、報酬「＋100」
　　を与える。
- 崖に落ちた場合（崖と定義されたマスに入った場合）、報酬「−100」を与える。
- 政策として「ε-Greedy手法」を採用する。
- 10000エピソードまでとする。
- ゴール地点Gに辿りつけず1000歩移動した場合、またはその歩数以内でゴール地点に辿りついた場合、1エピソード終了とする。
- エージェントは「1個体」のみとする。
- エージェントは「上下左右」の4方向へ1歩ずつ移動できるものとする。
- エージェントは自身のマップ上での位置を把握できるものとする＝完全状態・完全情報
- どのような状態表現とするかは「任意」。
- 乱数シード20個を用いて20試行実施する。

<div align="center">
    <img src=./figure/2018_acs_report_figure.png "課題設定">
    <div style="text-align: center;">
        <p>図1. 課題設定</p>
    </div>
</div>

## Demo

以下のような結果が得られます.
Q値の更新に次の行動を参照するSARSAは崖すれすれの場合, 行動決定がε-greedyのため崖から落ちる行動を参照する場合があります. その結果, 崖すれすれの部分を連続して通るような行動は避けるようにQ値が更新されます.
一方, QLearningは次のステップの最大のQ値を参照するためε-greedyにより崖から落ちることを考慮できません.その結果, 崖のそばを通り最短でゴールへ向かおうとします.

```
SARSA
|- - - - - - - - - - - - - - - - - - - |
|→ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → ↓ ↓ ↓ |
|→ ↓ ← ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ← ← |
|→ → → ↓ ↓ ↓ ↓ ↓ → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ← ← ← |
|→ → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ← ← ← ← |
|→ → → → ↑ ← → ↓ → → → → ← ↓ ↓ ← ← ← ← |
|→ ↑ → → ↑ ← → ↑ ← → → ↑ → → ← ← ← ← ↓ |
|→ → → → ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ← ← |
|→ → ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ← |
|→ → → ↑ ↑ ↑ ← ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ↓ |
|↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ↑ ↑ ↑ ↑ ↑ ↑ → ↓ |
|↑ - - - - - - - - - - - - - - - - - ↑ |
|- - - - - - - - - - - - - - - - - - - |

Q
|- - - - - - - - - - - - - - - - - - - |
|→ → ← → → ↓ ↓ ← ← → ↓ ↓ → ↓ → ← ← ← ↓ |
|↓ ← ↑ → ↓ → → ↓ ↓ ← → ↓ ↑ → → ↓ ← → ↓ |
|↓ ↑ ↑ ↓ ↑ → ↑ ↑ → → ↓ ↑ → ↑ ← ← ↓ → ↓ |
|↑ ← → ↓ → ↑ → ↑ → → ↓ ↓ ↓ ← ↓ → → ↓ ↓ |
|↓ ← ← ↑ ↓ ← ↑ → ← → → ↓ → → ← ↓ ↓ ↓ ↓ |
|→ → → ← → → → → ↓ ↓ → ↓ → ↓ → → → → ↓ |
|↓ ↓ ↑ → ↓ ↓ → → → → → ↓ → ↓ → ↓ → → ↓ |
|→ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → ↓ |
|→ → → → → → → → → → → → → ↓ → → ↓ → ↓ |
|→ → → → → → → → → → → → → → → → → → ↓ |
|↑ - - - - - - - - - - - - - - - - - → |
|- - - - - - - - - - - - - - - - - - - |
```

## Usage

c言語をコンパイルするためにgccが必要です.
```
# コンパイル
$ gcc reinforcement_learning.c  

# 実行
$ ./a.out
```

## Author

[takumi](https://github.com/i10bucchi)